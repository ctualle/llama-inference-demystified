# llama-inference-demystified
Rebuilding a LLaMa inference engine from scratch in Python. This project is a pedagogical and experimental implementation of a minimalist LLaMA inference engine capable of running LLaMA-2-7B quantized (Q8_0, GGUF format) without relying on llama.cpp, llama-cpp-python, or any existing inference backend. The goal is understanding, not performance.
